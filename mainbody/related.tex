\section{Related Work}
In this rection we introduce and compare some of the work most relevant to our method.

In \cite{lastLayerBayes}, the author uses the linear feature $\Lambda = \sum \phi\phi^\top+\lambda I$ to estimate the uncertainty of the Q-value estimate. They use the TS method, constructe covariance matrix $Conv=(\Lambda)^{-1}$ of the last layer w, then sampling on the distribution of $w\sim\mathcal{N}(\bar{w},Conv)$ and greedily adopting the largest action by  $a = \arg\max_{a\in\mathcal{A}}\phi(s,a)^\top w$. The same single-step bonus is derived, if converte the variance of the Gaussian distribution into UCB bonus $\beta[\phi^{\top}(\Lambda)^{-1}\phi]^{1/2}$ which is equivalent to this paper's work. The difference is that method proposed in \cite{lastLayerBayes} does not accumulate uncertainty through $H$. The effect of uncertainty only apply on one single step and is not passed through $\langle s, a, s' \rangle $. This means that the uncertainty in future steps cannot influence the decision of the current step, so that deep exploration cannot be achieved. This is reflected in his analysis of the regret world. In the case of $ H = 1 $, $ \text{Regret} (T) <C \sqrt {d ^ 2T} \log (T) $. But in the case of $ H> 1 $, the upper bound contains a $ \sqrt{\rho^H_\lambda} $ factor, which increases rapidly with the increase of H. This indicates that only by accumulating uncertainty can achieve right exploration.

In \cite{UBE} 's work, the same linear feature is also used to estimate the uncertainty of Q-value estimation and the TS method is applied as the exploration strategy. In their work, the uncertainty of the single-step Q estimation is accumulated by $ u^{target} = \phi ^ {\top} (\Lambda) ^ {-1} \phi + \gamma ^ 2 u (s ', a') $  which called Uncertainty Bellman 
Equation (UBE), which can be compared to an MDP with reward of $ \phi ^ {\top} (\Lambda) ^ {-1} \phi $, and a discount factor of $ \gamma ^ 2 $. The method proposed by \cite{UBE} is very close to ours, but apart from the differences in TS and UCB methods, the motivation is also different in source of uncertainty. Their method considers the uncertainty of the reward function and the transition function and uses the condition-independent property to derive the squared cumulative UBE, which is related to \textbf{Aleatoric Uncertainty}. Our method considers the error generated by fitting the MDP in the $\mathcal{R}^d$ space using the least squares method for finite number of observation samples, which is related to \textbf{Epistemic Uncertainty}. Due to the lack of analysis of the regret bound of the method proposed by \cite{UBE}, need more research to distinguish the difference between the two methods.

Our method is also related to the count-based method. Considering a tabular MDP with $ | {(s_i, a_i)} | = d $ where $(s_i,a_i)$ is the i-th state-action pair, we can set $ \phi (s_i, a_i) = [\phi_1, \phi_2, \cdots, \phi_d]^\top $ where $ \phi_j = \delta (i, j) $. We have 
\begin{eqnarray}
    && r(s_i, a_i) = \phi (s_i, a_i) ^ \top \theta\\
    && P(s_j | s_i, a_i) = \phi (s_i, a_i) ^ \top \mu (s_j) 
\end{eqnarray} 
 where $ \theta_i = r(s_i,a_i)$ and  where $ \mu (s_j)_i = P (s_j | s_i, a_i)$. Under these settings we have
 \begin{align}
 \Lambda &= \sum \phi \phi ^ \top + \lambda I \\
 &=diag (n_1 + \lambda, n_2 + \lambda, \cdots , n_n + \lambda)
 \end{align}
and 
 \[ \beta [\phi (s_i, a_i) ^ \top (\Lambda) ^ {-1} \phi (s_i, a_i)] ^ {1/2} = \frac{\beta} {\sqrt{n_i + \lambda}} \]
 which is equivalent to constructing a Model-Based Interval Estimation with Exploration Bonus (MBIE-EB) \cite{strehl2008analysis} using the exact count with $\lambda$ Laplacian smoothing. This example provides clues to the link between LiFE bonus and count-based bonus.

 There are some works based on count-based bonus construct density estimates $ \rho (s) $ and generate pseudocount based on 
 \[N (s) = \frac{\rho(s)(1-\rho'(s))}{\rho'(s)-\rho(s)}\]
 such as feature density model \cite{martin2017count}, neural density model \cite{ostrovski2017count}, successor representation \cite{machado2018count}, etc. The method in \cite{martin2017count} is the closest to our work , they consider that all the dimensions in the feature are independent other and constructs a density estimate by $\prod_{i=1}^d\rho_i(\phi_i(s))$, which is different from our linear feature approximation. These methods lack the guarantee of theoretical regret bounds, and may need to train additional neural networks and cause more computational overhead.