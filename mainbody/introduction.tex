\section{Introduction}
Reinforcement learning is a type of learning method that attempts to maximize the cumulative rewards an agent receives in the environment. In recent years, deep reinforcement learning algorithms using deep neural networks as function approximations have made impressive progress in solving continuous action control tasks, especially Actor-Critic algorithms such as TD3 \cite{TD3}, SAC \cite{SAC}, PPO \cite{PPO}, etc. These algorithms are widely used in tasks such as autonomous driving, drone flight, and robot control.

Reinforcement learning algorithms usually choose actions at each time step to maximize future cumulative reward based on the observed data; this is reflected in the optimal Bellman equation and the policy gradient. However, in more difficult tasks, the agent fails to improve the cumulative return or converge to the sub-optimal solution due to the lack of global data at the early stage of the training process, resulting in a non-efficient algorithm.

The most widely used exploration strategies in Actor-Critic algorithms are the indirectional exploration strategy, including $\epsilon$-greedy method \cite{DQN}, action-noise \cite{DDPG}, parameter-noise \cite{pnoise}, maximum entropy \cite{SQL}, etc. This type of method is easy to deploy but has no theoretical efficiency guarantee. The reason is that the indirectional method simply increases the probability of other non-optimal actions without specifying which states-actions need to be explored more. In principle, exploring states-actions with more uncertainty can explore the MDP more efficiently and make the algorithm converge to the global optimum.

Based on this principle, the Upper Confidence Bound (UCB) method \cite{auer2002finite,audibert2009exploration} and Thompson Sampling (TS) method \cite{TS,TStutorial} are proposed to design exploration strategies. The UCB method gives a higher additional bonus to state-actions with more uncertainty to explore these state-actions. The TS method samples value function from the posterior distribution based on the observed data and selects the optimal action so that the actions with more uncertainty are selected with a higher probability.

The UCB method and the TS method have been proved to be efficient in the case of tabular MDP \cite{TStutorial}, and have been implemented in Q-based algorithms on discrete action tasks \cite{BDQN,osband2018randomized}. However, the promotion of these exploration strategies in the Actor-critic algorithm on continuous action tasks is rare. Compared to Q-based algorithms, Actor-Critic algorithms are difficult to be compatible with TS directly. This is because TS algorithms usually sample on the posterior distribution, which results in the inconsistency of policy gradient estimated by sampled critics in each iteration step. According to this point, estimating the UCB and forming an exploration bonus is more suitable for Actor-Critic algorithm.

In recent works, the UCB method applied in the deep Actor-Critic algorithm mainly constructs a state density model $\rho(x)$ and induces the pseudocount and generates the exploration bonus by $N(s,a)^{-1/2}$ \cite{bellemare2016unifying,ostrovski2017count,machado2018count}. Among them, a state density estimation method is designed. The method of state density estimation is the core part of this kind of works. However, these works usually use heuristic design methods and lack theoretical efficiency guarantees. And it may be necessary to train additional network models, resulting in more computational overhead.

In this work, a method based on feature space pseudocount for forming the UCB bonus is proposed, which utilizes the ``$\phi(s,a)^\top w$'' structure of commonly used deep neural networks and references the theoretical analysis in Cai's work \cite{cai2019provably}. Our method can be plugged-in various state-of-the-art Actor-Critic algorithms. To demonstrate the performance of our method, a set of toy continuous action control task with different configurations are designed to verify that our algorithm. In addition, we modify the reward settings of several tasks in the gym \cite{gym} into sparse/deceptive reward versions and verify our method. The results show that the proposed algorithm can solve more complex sparse/deceptive reward tasks.

Here is an outline for the rest part of this article. In Section 2, the background relevant to this work is introduced. In Section 3, we cite the theoretical work of the regret bound and show how to implement Inverse-Cumulative-Feature Bonus to the Actor-Critic algorithms. In Section 4, we compare our method to the work most relevant to us. In Section 5, the results of empirical experiments on toy tasks and gym tasks are shown and explained. In the last section, we summarize this work and propose possible future work.
