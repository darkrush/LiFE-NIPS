\section{Background}
\subsection{Notation}
We consider an MDP with discount factor and deterministic reward: $\langle \mathcal{S},\mathcal{A},\mathcal{P},r,\gamma \rangle$, where $\mathcal{S}$ is the state space and $\mathcal{A}\subset \mathbb{R}^{d_a}$ is the continuous action space, $\mathcal{P}(s'|s,a)$ is the transition probability, $r(s,a)$ is a deterministic reward function and $\gamma$ is the discount factor.

A typical Actor-Critic algorithm for continuous action tasks includes a Q-value approximation function $Q(s,a):\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ and a policy function $\pi(a|s):\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R_+}$. In the context of the deep neural network they can be rewritten as $Q(s,a;\theta^{Q})$ and $\pi(a|s;\theta^{\pi})$. The commonly used Q-value neural network can be expressed as $Q(s,a;\theta^{Q}) = \phi(s,a;\theta^{\phi})^{\top}w$, in which $\phi(\cdot):\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^d$ is a non-linear mapping named "feature mapping" and $\phi^{\top}w$ is a linear mapping.

\subsection{Sparse and Deceptive Reward}
\label{sec:tasks}
Usually, a reward for a task is defined according to the needs of the application. For example, in the HalfCheetah task \cite{mujoco}, a half-body cheetah with 6 degrees of freedom is placed on the ground plane. In each time step, the reward is $r_t = r_{speed} + r_{control}$, where $r_{speed}$ is proportional to the distance to the right in this time step, and $r_{control} = -\alpha*||\vec{a_t}||^2_2$ is the action penalty. The implication behind the reward is that the agent is expected to achieve higher speed with minimal control cost. Since the agent is constrained to a two-dimensional vertical plane, any random action can cause the agent to move to the right and immediately get positive feedback. It is easy to sample a policy that can get a positive reward from policy space, and the positive reward will further drive the agent to learn how to run faster, thus forming positive feedback.

However, this well-defined situation does not always appear in all tasks, and positive feedback may not be easy to reach. In some tasks where the goal is to reach specific target states, a positive reward can only be obtained when the agent reaches these states. It means that the agent can only get 0 rewards before it happens to touch these states. The $Q(s,a)$ or $V(s)$ will learn to equal zero for any state and action, and make the policy gradient equal zero, the actor cannot be optimized anymore. In addition, in the control task, in order to reduce the energy consumed by controlling each joint, the action penalty term (as $r_{control}$ in HalfCheetah) will be included in the reward. Before getting any positive feedback, the agent will learn to keep the zero action as the best policy to minimize the penalty, and this further prevents the agent from making various exploratory actions. "Sparse reward" and "zero action" together led to the morass of exploration. In these cases, additional exploration strategies become especially important. Once the agent reaches the goal states and gains a positive reward through an additional exploration strategy, the critic will direct the actor to learn to access the goal states with a higher frequency and finally learn to solve the task.

\subsection{Exploration Strategy}
Among the current exploration strategies for model-free RL, indirectional exploration strategies are often used due to the convenience of deployment, including $\epsilon$ -greedy \cite{DQN}, action-noise \cite{DDPG}, parameter-noise \cite{pnoise}, maximum entropy methods \cite{SQL}, etc. This type of method has the problem of being inefficient for long horizon problems, in other words, it cannot achieve "deep exploration" \cite{osband2018randomized}. Achieving effective exploration requires following the "optimistic" principle: assigning higher visit probability to choices with more uncertainty. However, the indirectional method does not use the information of the past access status, and cannot correctly assign a higher probability to a choice with more uncertainty.

The TS method is a type of exploration algorithm that samples the posterior distribution of the value model and greedily chooses the optimal action based on the sampling results \cite{TStutorial}. Since the posterior sampling model replaces the maximum likelihood model, there is a higher probability of selecting a suboptimal action with large uncertainty. The key point of this type of work is how to sample the value model on the posterior distribution. Existing works include \cite{BDQN, osband2018randomized, lastLayerBayes}. However, for the Actor-Critic algorithms, the policy follows the critic by accumulating the policy gradient slowly, and is not able to reflect the results of the critical posterior sampling in time, which makes TS method difficult to be applied in Actor-Critic algorithms.

Different from the TS method, the UCB method uses the upper confidence bound of the posterior distribution of the value model to replace the maximum likelihood model and selects the action greedily to achieve exploration \cite{auer2002finite,audibert2009exploration}. A standard method of this type of method is to give an estimate of the uncertainty of the value model as a bonus added to the original reward of the task. Since the bonus changes slowly with the number of training rounds, the effect on actors during a training session is consistent. Compared with TS method, UCB method is more compatible with Actor-Critic method.